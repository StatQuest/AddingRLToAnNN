{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d7001c2-12c6-4f02-ba83-bcdb85073390",
   "metadata": {},
   "source": [
    "# StatQuest: Adding Reinforcement Learning to a Neural Network\n",
    "\n",
    "Copyright 2025, Joshua Starmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33af8301-514e-4a9c-bf09-de9bc65bfb04",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e6f01f-e856-42d6-bcc4-9a5c844fda94",
   "metadata": {},
   "source": [
    "In this tutorial we illustrate how to add **Reinforcement Learning** (**RL**) to a simple **Neural Network**. Specifically, we illustrate how to add the **Policy Gradients** **RL** method to a **Neural Network** that is designed to help us decide where to go eat fries, either **'Squatch's Fry Shack** or **Norm's Fry Hut**, based on how hungry we are. If we are very hungry, than we will want to go to the place that has a high probability of giving us a large order of fries. However, if we are not very hungry, we want to go to the place that has a low probability of giving us a large order of fries. Also, to a little bit interesting, there is a **20%** chance that **'Squatch** will give us a large order of fries and an **80%** chance that **Norm** will give us a large order of fries. \n",
    "\n",
    "This example is based on the StatQuest videos: \n",
    "- **[Reinforcement Learning with Neural Networks: Essential Concepts!!!](https://youtu.be/9hbQieQh7-o)**\n",
    "- **[Reinforcement Learning with Neural Networks: Mathematical Details](https://youtu.be/DVGmsnxB2UQ)**\n",
    "\n",
    "<img src=\"./images/nn_plus_fries.png\" alt=\"the neural network that we will implement\" style=\"width: 800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5640bfe-6875-420e-b1b2-f636d158973c",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdced778-c708-4012-8aa0-9dacecba1cca",
   "metadata": {},
   "source": [
    "# Import the modules that will do all the work\n",
    "\n",
    "The very first thing we need to do is load a bunch of Python modules. Python itself is just a basic programming language. These modules give us extra functionality to generate random numbers and create and train a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfe188c-92a7-4504-8dd7-6e03b05ed804",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# %%capture prevents this cell from printing a ton of STDERR stuff to the screen\n",
    "\n",
    "## NOTE: If you **don't** need to install anything, you can comment out the\n",
    "##       next line.\n",
    "##\n",
    "##       If you **do** need to install something, just know that you may need to\n",
    "##       restart your session for python to find the new module(s).\n",
    "##\n",
    "##       To restart your session:\n",
    "##       - In Google Colab, click on the \"Runtime\" menu and select\n",
    "##         \"Restart Session\" from the pulldown menu\n",
    "##       - In a local jupyter notebook, click on the \"Kernel\" menu and select\n",
    "##         \"Restart Kernel\" from the pulldown menu\n",
    "##\n",
    "##       Also, installing can take a few minutes, so go get yourself a snack!\n",
    "!pip install lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363763f5-3b4b-454f-a10c-ec9f78c921cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch ## torch let's us create tensors and also provides helper functions\n",
    "import torch.nn as nn ## torch.nn gives us nn.Module(), nn.Embedding() and nn.Linear()\n",
    "import torch.nn.functional as F # This gives us sigmoid()???\n",
    "from torch.optim import Adam # optim contains many optimizers. This time we're using Adam\n",
    "\n",
    "import lightning as L ## Lightning makes it easier to write, optimize and scale our code\n",
    "from torch.utils.data import TensorDataset, DataLoader ## We'll store our data in DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fb14c9-c08b-48b6-94ac-8547ce187000",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27d8c5e-2387-455f-baad-aec154a3eba2",
   "metadata": {},
   "source": [
    "# Create Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d564c6f-fa6e-4289-bfaf-83ec3e099aed",
   "metadata": {},
   "source": [
    "Training data? I thought this was **Reinforcement Learning**. Why do we need training data?\n",
    "\n",
    "From the perspective of doing **Reinforcement Learning**, we don't need training data, since the model itself does the exploring. Our model will create its own inputs and discover the corresponding \"labels\". Then it will use a reward to modify the \"label\" depending on whether or not it was correct.\n",
    "\n",
    "That said, the **Lightning Trainer** needs something it can iterate in order to train the model. So, here we will create some data that we will, ultimately,  ignore in our model's `training_step()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8be4198-3295-459f-a678-d1e139c1c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## In order ot use the Lightning Trainer, \n",
    "## we need to give it something it can iterate on.\n",
    "## So here is some data that we will completely ignore in the\n",
    "## training_step() method.\n",
    "training_inputs = torch.tensor([0.8, 0.1, 0.2, 0.9])\n",
    "training_labels = torch.tensor([1.0, 0.0, 0.0, 1.0])\n",
    "\n",
    "## Now let's package everything up into a DataLoader...\n",
    "training_dataset = TensorDataset(training_inputs, training_labels) \n",
    "dataloader = DataLoader(training_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3845d2-7b41-493b-b7b3-83e75d45a7d7",
   "metadata": {},
   "source": [
    "Now that we have a dataloader, we can create the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d698d120-c55c-4250-8b08-8780cff8a39d",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dc5a58-4ace-4c07-aca8-fb782dfdffd0",
   "metadata": {},
   "source": [
    "# Create a simple Neural Network with a trainable Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a532d408-1630-456d-af2b-58babddc9b16",
   "metadata": {},
   "source": [
    "Because the goal is to illustrate how to add **Reinforcement Learning** to a neural network, and not how to create a really fancy neural network, we're going to use an incredibly simple neural network that has a single trainable parameter, a **Bias** term. And we'll use **Reinforcement Learning** to train that one **Bias**. That said, the method easily extends as many parameters we need to train if we, ultimately, create a fancy neural network.\n",
    "\n",
    "<img src=\"./images/just_nn.png\" alt=\"the neural network that we will implement\" style=\"width: 800px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62a91bc-fead-4af1-b4bf-494a7840d059",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNN_with_RL(L.LightningModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        L.seed_everything(seed=42)\n",
    "        \n",
    "        self.weight = torch.tensor(20) # We won't train this weight.\n",
    "\n",
    "        ## However, we will train this bias.\n",
    "        ## The ideal value sis -10.\n",
    "        self.bias = nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "        ## gamma is like a learning rate applied directly to the reward.\n",
    "        self.gamma = torch.tensor(0.99)\n",
    "        \n",
    "        ## We need this to keep track of the reward.\n",
    "        self.reward = torch.tensor(0) \n",
    "\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        ## A forward pass through a super simple neural network\n",
    "        ## NOTE: In reinforcement learning lingo, a neural network\n",
    "        ##       that we want to train is called a \"policy network\".\n",
    "\n",
    "        p_norm = torch.sigmoid(inputs * self.weight + self.bias)\n",
    "\n",
    "        return p_norm\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self): \n",
    "        ## Configure the optimizer we want to use for backpropagation.\n",
    "        return Adam(self.parameters(), lr=0.1)\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx): \n",
    "        ## Take a step during gradient descent with policy gradients.\n",
    "\n",
    "        ## First, decide how hungry we are...\n",
    "        ## NOTE: if we were not forcing all of the training\n",
    "        ##       to take place on a CPU, then we would need\n",
    "        ##       to move how_hungry to the GPU or whatever\n",
    "        ##       we are using to accelerate training. We\n",
    "        ##       can do that by adding .to(\"gpu\"), or .to(\"mps\") on a mac,\n",
    "        ##       to the value like this: torch.rand(1).to(\"gpu\")\n",
    "        #how_hungry = torch.rand(1).to(\"mps\")\n",
    "        how_hungry = torch.rand(1)\n",
    "        \n",
    "        ## Now pass how_hungry through the neural network to get a probability\n",
    "        ## for going to Norm's.\n",
    "        ## NOTE: In reinforcement learning lingo, the neural network\n",
    "        ##       we want to train is called a \"policy network\".\n",
    "        outputs = self.forward(how_hungry)   \n",
    "        # print(\"outputs:\", outputs)\n",
    "        \n",
    "        ## now figure out if we go to Norm's or Squatch's\n",
    "        ## by picking a random number between 0 and 1...\n",
    "        ## NOTE: We would need to move rand_num to the \n",
    "        ##       accelerator as well (if we were not forcing\n",
    "        ##       all training on the CPU)\n",
    "        #rand_num = torch.rand(1).to(\"mps\")\n",
    "        rand_num = torch.rand(1)\n",
    "        # print(\"rand_num:\", rand_num)\n",
    "\n",
    "        ## ...and then comparing that number to the output\n",
    "        ## from the neural network (ahem, \"policy network\").\n",
    "        ## If the random number is < the output, go to Norm's.\n",
    "        if(rand_num < outputs):\n",
    "            ## go to norm's\n",
    "            # print(\"\\tgoing to norms!\")\n",
    "            \n",
    "            ## Now determine if Norm is giving us a large order or not\n",
    "            if (torch.rand(1) < 0.8): # Norm gave us a large order...\n",
    "\n",
    "                ## if how_hungry is > 0.5, then we are hungry and want a larger order...\n",
    "                if(how_hungry > 0.5): # \n",
    "                    self.reward = 1 # We are hungry and happy we got a large order\n",
    "                else:\n",
    "                    self.reward = -1 # We are not hungry and not happy we got a large\n",
    "            else: # Norm gave us a small order\n",
    "                if(how_hungry > 0.5):\n",
    "                    self.reward = -1  # We are hungry and sad we got a small order\n",
    "                else:\n",
    "                    self.reward = 1 # We are not hungry and happy we got a small order\n",
    "        else:\n",
    "            ## go to squatch's\n",
    "            # print(\"\\tgoing to squatch!\")\n",
    "\n",
    "            # Now convert the probability of visiting Norm to the probability of visiting Squatch\n",
    "            outputs = 1 - outputs\n",
    "            \n",
    "            ## Now determine if Squatch is giving us a large order or not\n",
    "            if (torch.rand(1) < 0.2): # Squatch gave us a large order...\n",
    "                if(how_hungry > 0.5): \n",
    "                    self.reward = 1 # We are hungry and happy squatch gave us a large order\n",
    "                else:\n",
    "                    self.reward = -1 # We are not hungry and sad we got a large order\n",
    "            else: # Squatch gave us a small order\n",
    "                if(how_hungry > 0.5): \n",
    "                    self.reward = -1 # We are hungry and sad squatch gave us a small order\n",
    "                else:\n",
    "                    self.reward = 1 # We are not hungry and happy we got a small order\n",
    "\n",
    "        ## Now that we have visited either Norm or 'Squatch and ordered fries,\n",
    "        ## we can calculate the loss.\n",
    "        ## NOTE: We're using cross entropy which is...\n",
    "        ## CE_squatch = -1 * log(1-output) * gamma * reward\n",
    "        ## CE_norm = -1 * log(output) * gamma * reward\n",
    "        ## When pytorch calculates the derivative of the cross entropy\n",
    "        ## the scaled reward still have the desired effect.\n",
    "        loss = -1 * torch.log(outputs) * self.gamma * self.reward\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75d5cdc-2183-40ff-a1a6-01fc87383a35",
   "metadata": {},
   "source": [
    "### Bam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5646fd-9d00-4253-b76e-e4f840fa57bb",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6553196a-0516-4d55-99b5-284dabe2fd88",
   "metadata": {},
   "source": [
    "# Run some input values through neural network to see what it does before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84029290-4e90-4b7f-9b3c-ac85038f1a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simpleNN_with_RL()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbb1679-b65e-48a6-87c5-0d06d0fd751f",
   "metadata": {},
   "source": [
    "First, let's run a relatively large input value, **0.9**, through the neural network. A relatively large input value (a value close to **1**) indicates that we are hungry and would like to go somewhere that has a good chance of giving us a large order of fries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a939a19-0424-4ebc-a161-8f1b66a377be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(torch.tensor(0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b00328e-d121-452b-b5b0-b2e0c2e43268",
   "metadata": {},
   "source": [
    "The output, **0.9995**, means there is a high probability that we will go to **Norm's** to get fries. This makes sense because we are hungry (the input was **0.9**) and there a good chance that **Norm** will give us a large order of fries.\n",
    "\n",
    "Now let's run a relatively small input value, **0.1**, thorugh the neural network. A relatively small input value (a value close to **0**) indicates that we are not hungry and would like to go somewhere that has a good chance of serving us a small order of fries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b523bc-f87c-438f-b916-575ec5125bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(torch.tensor(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8fcedc-f660-482c-a961-e00e268217fa",
   "metadata": {},
   "source": [
    "The output, **0.8808**, means there is a high probability that we will go to **Norm's** to get fries. This does not make sense because we are not hungry (the input was **0.1**) and there is a good chance that **Norm** will give us a large order of fries. This means we'll waste fries, and that is no good at all. This means we need to train our model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf12a7da-36de-4dd3-aac6-5944a8d5b9d7",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3556dc52-7c91-46da-a7ca-60d24db5d46d",
   "metadata": {},
   "source": [
    "# Now train the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043b59f1-e788-4e90-bc94-cb200532ac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simpleNN_with_RL()\n",
    "trainer = L.Trainer(max_epochs=70, accelerator=\"cpu\")\n",
    "trainer.fit(model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e71915-10b6-473b-b926-6568d43a6f59",
   "metadata": {},
   "source": [
    "Now that we've trained the model, we can re-run the input values we used before to see if the outputs have changed. We'll start with a relatively high input value, **0.9**, that indicates that we are hungry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c81839d-d026-4694-8a7a-f20f4d129002",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(torch.tensor(0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b39082e-23c6-4cae-acf8-0838117733ba",
   "metadata": {},
   "source": [
    "The result, **1**, means there is a **100%** chance that we will go to **Norm's**. And this makes sense because we are hungry, the input values was 0.9, and there's a good chance that Norm will give us a lot of fries.\n",
    "\n",
    "Now let's see what happens when we use a relatively small input value, **0.1**, that indicates that we are not hungry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb54f6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(torch.tensor(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9d7543-c8ee-4f39-88df-450f0131ec6f",
   "metadata": {},
   "source": [
    "Unlike before we trained the model, now the output is close to 0, meaning there is low probability that we will go to Norm's. This means there is a high probabilty that we will go to 'Squatch's. And that makes sense, because there is a good chance that 'Squatch will eat most of the fries and only give us a small order. This means we will not waste any fries, and that is a good thing!\n",
    "\n",
    "## DOUBLE BAM!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ccd1b8-2fce-41cf-8c79-51a219a50370",
   "metadata": {},
   "source": [
    "Now let's print out all of the trainable parameters, meaning, the one **Bias**, to see what the value is, after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48765b1a-cc77-4517-91c4-779bc7aef44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, torch.round(param.data, decimals=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16a630d-a55a-43eb-9a85-9fdf4f4df097",
   "metadata": {},
   "source": [
    "That value, **-10.3**, is very close to the ideal value, **-10**, and that means that **Reinforcement Learning** successfully trained the model.\n",
    "\n",
    "# TRIPLE BAM!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
