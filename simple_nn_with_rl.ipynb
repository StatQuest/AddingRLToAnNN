{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d7001c2-12c6-4f02-ba83-bcdb85073390",
   "metadata": {},
   "source": [
    "# THIS MODEL USES REINFORCEMENT LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdced778-c708-4012-8aa0-9dacecba1cca",
   "metadata": {},
   "source": [
    "# Import modules..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "363763f5-3b4b-454f-a10c-ec9f78c921cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch ## torch let's us create tensors and also provides helper functions\n",
    "import torch.nn as nn ## torch.nn gives us nn.Module(), nn.Embedding() and nn.Linear()\n",
    "import torch.nn.functional as F # This gives us sigmoid()???\n",
    "from torch.optim import Adam # optim contains many optimizers. This time we're using Adam\n",
    "\n",
    "import lightning as L ## Lightning makes it easier to write, optimize and scale our code\n",
    "from torch.utils.data import TensorDataset, DataLoader ## We'll store our data in DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27d8c5e-2387-455f-baad-aec154a3eba2",
   "metadata": {},
   "source": [
    "# Create training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c8be4198-3295-459f-a678-d1e139c1c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We don't need training data for reinforcement learning\n",
    "## Since the model itself does the exploring. So it creates\n",
    "## the inputs and discovers the \"labels\" and a reward\n",
    "## tells us if we got the right \"label\" or not.\n",
    "\n",
    "training_inputs = torch.tensor([0.8, 0.1, 0.2, 0.9])\n",
    "\n",
    "training_labels = torch.tensor([1.0, 0.0, 0.0, 1.0])\n",
    "\n",
    "## Now let's package everything up into a DataLoader...\n",
    "training_dataset = TensorDataset(training_inputs, training_labels) \n",
    "dataloader = DataLoader(training_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dc5a58-4ace-4c07-aca8-fb782dfdffd0",
   "metadata": {},
   "source": [
    "# Create a Neural Network with a Trainable Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f62a91bc-fead-4af1-b4bf-494a7840d059",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNN_with_RL(L.LightningModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        L.seed_everything(seed=42)\n",
    "        \n",
    "        self.weight = torch.tensor(20)\n",
    "        self.bias = nn.Parameter(torch.tensor(0.0)) ## The ideal value for the bias is -10\n",
    "\n",
    "        \n",
    "        self.gamma = torch.tensor(0.99)\n",
    "        self.reward = torch.tensor(0)\n",
    "\n",
    "        # self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        ## A forward pass through a super simple neural network\n",
    "\n",
    "        p_norm = torch.sigmoid(inputs * self.weight + self.bias)\n",
    "\n",
    "        return p_norm\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self): \n",
    "        ## Configure the optimizer we want to use for backpropagation.\n",
    "        return Adam(self.parameters(), lr=0.1)\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx): # take a step during gradient descent.\n",
    "        ## NOTE: When training_step() is called it calculates the loss with the code below...\n",
    "        # inputs, labels = batch # collect input\n",
    "        # outputs = self.forward(inputs) # run input through the neural network\n",
    "\n",
    "        ## First, decide how hungry we are...\n",
    "        how_hungry = torch.rand(1).to(\"mps\")\n",
    "        ## Now use how_hungry to get a probability for going to Norm's from the policy network...\n",
    "        outputs = self.forward(how_hungry)\n",
    "        \n",
    "        # print(\"outputs:\", outputs)\n",
    "        ## now figure out if we go to Norm's or Squatch's\n",
    "        rand_num = torch.rand(1).to(\"mps\")\n",
    "        # print(\"rand_num:\", rand_num)\n",
    "        if(rand_num < outputs):\n",
    "            ## go to norm's\n",
    "            # print(\"\\tgoing to norms!\")\n",
    "            ## Now determine if norm is giving us a large order or not\n",
    "            if (torch.rand(1) < 0.8): # Norm gave us a large order...\n",
    "                if(how_hungry > 0.5): # \n",
    "                    self.reward = 1 # We are hungry and happy we got a large order\n",
    "                else:\n",
    "                    self.reward = -1 # We are not hungry and not happy we got a large\n",
    "            else: # Norm gave us a small order\n",
    "                if(how_hungry > 0.5):\n",
    "                    self.reward = -1  # We are hungry and sad we got a small order\n",
    "                else:\n",
    "                    self.reward = 1 # We are not hungry and happy we got a small order\n",
    "        else:\n",
    "            ## go to squatch's\n",
    "            # print(\"\\tgoing to squatch!\")\n",
    "            outputs = 1 - outputs # convert to probability of visiting Squatch\n",
    "            ## Now determine if Squatch is giving us a large order or not\n",
    "            if (torch.rand(1) < 0.2): # Squatch gave us a large order...\n",
    "                if(how_hungry > 0.5): \n",
    "                    self.reward = 1 # We are hungry and happy squatch gave us a large order\n",
    "                else:\n",
    "                    self.reward = -1 # We are not hungry and sad we got a large order\n",
    "            else: # Squatch gave us a small order\n",
    "                if(how_hungry > 0.5): \n",
    "                    self.reward = -1 # We are hungry and sad squatch gave us a small order\n",
    "                else:\n",
    "                    self.reward = 1 # We are not hungry and happy we got a small order\n",
    "\n",
    "\n",
    "        ## convert outputs for norm's in to probabilities for norm's\n",
    "        # outputs = (labels * outputs) + ((1 - labels) * (1 - outputs))\n",
    "        # loss = -torch.log(action_probs[0, action]) * total_reward\n",
    "            \n",
    "        loss = -1 * torch.log(outputs) * self.gamma * self.reward\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6553196a-0516-4d55-99b5-284dabe2fd88",
   "metadata": {},
   "source": [
    "# Run the inputs through neural network to make sure forward() works..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "84029290-4e90-4b7f-9b3c-ac85038f1a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "model = simpleNN_with_RL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e30e9b5f-81f0-48fb-83c3-f3402d04d95f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8000, 0.1000, 0.2000, 0.9000])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "22288c04-cd8d-480d-8a8e-d4ea8e4a6ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 0.8808, 0.9820, 1.0000], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(training_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3556dc52-7c91-46da-a7ca-60d24db5d46d",
   "metadata": {},
   "source": [
    "# Now train the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "043b59f1-e788-4e90-bc94-cb200532ac3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params | Mode\n",
      "---------------------------------------------\n",
      "  | other params | n/a  | 1      | n/a \n",
      "---------------------------------------------\n",
      "1         Trainable params\n",
      "0         Non-trainable params\n",
      "1         Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e052ed15f8d4d0096a73c8556e967f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=70` reached.\n"
     ]
    }
   ],
   "source": [
    "model = simpleNN_with_RL()\n",
    "trainer = L.Trainer(max_epochs=70)\n",
    "trainer.fit(model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d281603b-e69c-4e27-a8a5-c30b1dbf0c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.9654e-01, 2.3959e-04, 1.7676e-03, 9.9953e-01],\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(training_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "48765b1a-cc77-4517-91c4-779bc7aef44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias tensor(-10.3400)\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, torch.round(param.data, decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b055b4b6-1d78-41af-a8f2-75733fda93bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9999, grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9ec89e19-4429-42e2-9f23-8c11fe28377e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4167, grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "30c09d75-79a4-4b50-9124-10cc9e45d218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2431e-05, grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665397cd-6021-4f6f-8f8b-f00b0a07f213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f2e45a-c8e3-495d-b925-ac21feded28b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e9576f-2890-440b-a327-1616b3a51fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c34a483-a57f-41a3-b82a-e647a4c4d2b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Define a simple neural network for our agent\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mPolicyNetwork\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "## THIS IS AN EXAMPLE GOOGLE GENERATED\n",
    "## It uses the REINFORCE algorithm, which maximizes the expected reward.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "\n",
    "# Define a simple neural network for our agent\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return torch.softmax(x, dim=-1)\n",
    "\n",
    "# Create an environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Define hyperparameters\n",
    "learning_rate = 0.01\n",
    "gamma = 0.99\n",
    "\n",
    "# Initialize the policy network and optimizer\n",
    "policy_net = PolicyNetwork(env.observation_space.shape[0], env.action_space.n)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for episode in range(1000):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    while True:\n",
    "        # Choose an action based on the current policy\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        action_probs = policy_net(state_tensor)\n",
    "        action = torch.multinomial(action_probs, 1).item()\n",
    "\n",
    "        # Take the action in the environment\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        # Update the policy network\n",
    "        if done:\n",
    "            loss = -torch.log(action_probs[0, action]) * total_reward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            break\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode: {episode}, Total Reward: {total_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
